{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Festivalle MER.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "YcSGRhIejyyp",
        "QdccWOClj2ZM",
        "RpsvbLjhOHqL",
        "9FMgh24mcgw1"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PSlSbiddcp_"
      },
      "source": [
        "# import "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6U5xvhLUsTJ"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import TimeDistributed, Input, Conv1D, GlobalMaxPooling1D, MaxPooling1D, Bidirectional, BatchNormalization, Dropout, Add, LSTM, Dense\n",
        "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
        "!pip install tensorflow_addons\n",
        "import tensorflow_addons as tfa\n",
        "from sklearn.model_selection import KFold\n",
        "from keras import backend as k\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import pathlib\n",
        "import os\n",
        "import urllib.request\n",
        "from zipfile import ZipFile \n",
        "import warnings\n",
        "import librosa\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import load_model\n",
        "\n",
        "plt.style.use('seaborn');\n",
        "\n",
        "\n",
        "import itertools\n",
        "plt.style.use('seaborn');\n",
        "\n",
        "print(tf.__version__)\n",
        "print(tf.test.gpu_device_name())\n",
        "\n",
        "!nvidia-smi\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB4mH4mDdkBL"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcSGRhIejyyp"
      },
      "source": [
        "### Load mp3 files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQLsZX0YJIaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5de4b311-c5d9-4d89-a88b-3e74dee5462b"
      },
      "source": [
        "!mkdir audiofiles\n",
        "!mkdir labelled_data\n",
        "!wget --no-check-certificate -r \"http://cvml.unige.ch/databases/DEAM/DEAM_audio.zip\" -O \"audiofiles/Input.zip\" "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: combining -O with -r or -p will mean that all downloaded content\n",
            "will be placed in the single file you specified.\n",
            "\n",
            "--2021-06-24 13:43:20--  http://cvml.unige.ch/databases/DEAM/DEAM_audio.zip\n",
            "Resolving cvml.unige.ch (cvml.unige.ch)... 129.194.10.44\n",
            "Connecting to cvml.unige.ch (cvml.unige.ch)|129.194.10.44|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://cvml.unige.ch/databases/DEAM/DEAM_audio.zip [following]\n",
            "--2021-06-24 13:43:20--  https://cvml.unige.ch/databases/DEAM/DEAM_audio.zip\n",
            "Connecting to cvml.unige.ch (cvml.unige.ch)|129.194.10.44|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1343203527 (1.2G) [application/zip]\n",
            "Saving to: ‘audiofiles/Input.zip’\n",
            "\n",
            "audiofiles/Input.zi 100%[===================>]   1.25G  28.9MB/s    in 47s     \n",
            "\n",
            "2021-06-24 13:44:07 (27.5 MB/s) - ‘audiofiles/Input.zip’ saved [1343203527/1343203527]\n",
            "\n",
            "FINISHED --2021-06-24 13:44:07--\n",
            "Total wall clock time: 48s\n",
            "Downloaded: 1 files, 1.2G in 47s (27.5 MB/s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxoL2PTvPMPq"
      },
      "source": [
        "zipname = \"audiofiles/Input.zip\"\n",
        "with ZipFile(zipname,'r') as zpfile:\n",
        "  files = zpfile.namelist()\n",
        "  for f in files:\n",
        "    if (f.endswith('.mp3')):\n",
        "      zpfile.extract(f, 'audiofiles')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdccWOClj2ZM"
      },
      "source": [
        "## load annotations\n",
        "Annotations per second. In the first line we can retrieve the sample number at which they were calculated in the format sample_xxxxxms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A26DOMMqeY_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7558e9e0-4884-4dcb-b0bc-e980fe9f16e1"
      },
      "source": [
        "!wget --no-check-certificate -r \"https://drive.google.com/uc?export=download&id=1JA5dpdmP8TEh4sOfW40GcyNlD_t0K2EM\" -O \"labelled_data/arousal.csv\"\n",
        "!wget --no-check-certificate -r \"https://drive.google.com/uc?export=download&id=1_PVma3Eb4YleUHQgmk6Ekjs5k7Pr6uWx\" -O \"labelled_data/valence.csv\"\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: combining -O with -r or -p will mean that all downloaded content\n",
            "will be placed in the single file you specified.\n",
            "\n",
            "--2021-06-24 13:44:18--  https://drive.google.com/uc?export=download&id=1JA5dpdmP8TEh4sOfW40GcyNlD_t0K2EM\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.164.174, 2607:f8b0:4004:815::200e\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.164.174|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0o-38-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qj2m08bnhg87tl5r7v8r9ikhm1fdecop/1624542225000/02572735245738743972/*/1JA5dpdmP8TEh4sOfW40GcyNlD_t0K2EM?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-06-24 13:44:20--  https://doc-0o-38-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qj2m08bnhg87tl5r7v8r9ikhm1fdecop/1624542225000/02572735245738743972/*/1JA5dpdmP8TEh4sOfW40GcyNlD_t0K2EM?e=download\n",
            "Resolving doc-0o-38-docs.googleusercontent.com (doc-0o-38-docs.googleusercontent.com)... 142.250.73.193, 2607:f8b0:4004:829::2001\n",
            "Connecting to doc-0o-38-docs.googleusercontent.com (doc-0o-38-docs.googleusercontent.com)|142.250.73.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1029849 (1006K) [text/csv]\n",
            "Saving to: ‘labelled_data/arousal.csv’\n",
            "\n",
            "labelled_data/arous 100%[===================>]   1006K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-06-24 13:44:20 (20.3 MB/s) - ‘labelled_data/arousal.csv’ saved [1029849/1029849]\n",
            "\n",
            "FINISHED --2021-06-24 13:44:20--\n",
            "Total wall clock time: 1.8s\n",
            "Downloaded: 1 files, 1006K in 0.05s (20.3 MB/s)\n",
            "WARNING: combining -O with -r or -p will mean that all downloaded content\n",
            "will be placed in the single file you specified.\n",
            "\n",
            "--2021-06-24 13:44:20--  https://drive.google.com/uc?export=download&id=1_PVma3Eb4YleUHQgmk6Ekjs5k7Pr6uWx\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.164.174, 2607:f8b0:4004:815::200e\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.164.174|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0k-38-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/e09qs3295k1sq8lu5c222udhtl4mgilc/1624542225000/02572735245738743972/*/1_PVma3Eb4YleUHQgmk6Ekjs5k7Pr6uWx?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-06-24 13:44:21--  https://doc-0k-38-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/e09qs3295k1sq8lu5c222udhtl4mgilc/1624542225000/02572735245738743972/*/1_PVma3Eb4YleUHQgmk6Ekjs5k7Pr6uWx?e=download\n",
            "Resolving doc-0k-38-docs.googleusercontent.com (doc-0k-38-docs.googleusercontent.com)... 142.250.73.193, 2607:f8b0:4004:829::2001\n",
            "Connecting to doc-0k-38-docs.googleusercontent.com (doc-0k-38-docs.googleusercontent.com)|142.250.73.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1031593 (1007K) [text/csv]\n",
            "Saving to: ‘labelled_data/valence.csv’\n",
            "\n",
            "labelled_data/valen 100%[===================>]   1007K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-06-24 13:44:22 (21.2 MB/s) - ‘labelled_data/valence.csv’ saved [1031593/1031593]\n",
            "\n",
            "FINISHED --2021-06-24 13:44:22--\n",
            "Total wall clock time: 1.8s\n",
            "Downloaded: 1 files, 1007K in 0.05s (21.2 MB/s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXQovFOnoAj1"
      },
      "source": [
        "arousal = pd.read_csv('labelled_data/arousal.csv')\n",
        "valence = pd.read_csv('labelled_data/valence.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GfNkPCKoIKg"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygCJumoAM9Op"
      },
      "source": [
        "Some files appeared corrupted, missing some samples. So we get rid of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTzUk1fioJgy"
      },
      "source": [
        "black_list = ['435', '990', '146',\n",
        "              '272', '1273', '1200',\n",
        "              '1174',\n",
        "              '1493', '1789', ]\n",
        "for r in black_list:\n",
        "  os.remove('audiofiles/MEMD_audio/' + str(r) + '.mp3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brPyuroeNLJk"
      },
      "source": [
        "We create a list of all the paths to the mp3 files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1c3TqtQNdYd"
      },
      "source": [
        "audio_paths = [str(file) for file in Path().glob('./audiofiles/MEMD_audio/*.mp3')]\n",
        "SR = 44100 # 1/s\n",
        "SEGMENT_DURATION = 0.5 # s\n",
        "N_BUFFERS = 60 # per song\n",
        "BUF_LENGTH = int(SEGMENT_DURATION * SR)\n",
        "\n",
        "\n",
        "def load_files(data, labels, paths, unique_ids):\n",
        "    for i, path in enumerate(paths):\n",
        "        audio, _ = librosa.load(path, sr=SR)\n",
        "        audio = audio[15*SR:45*SR]\n",
        "        audio = librosa.utils.normalize(audio)\n",
        "        id = path.split('.mp3')[0].split('/')[2] if i<unique_ids else path.split('.mp3')[0].split('/')[4].split('-')[0]\n",
        "        for j, buf in enumerate(range(N_BUFFERS)):\n",
        "            v = valence[valence['song_id']== int(id)].values[0,buf+1]\n",
        "            a = arousal[arousal['song_id']== int(id)].values[0,buf+1]\n",
        "            labels[i*N_BUFFERS + j] = np.array([a,v]).reshape(-1,1)\n",
        "            data[i*N_BUFFERS + j] = (audio[buf*BUF_LENGTH : (buf+1)*BUF_LENGTH]).reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKRZhlrrNmb3"
      },
      "source": [
        "data_size = len(audio_paths)*60\n",
        "\n",
        "X = np.empty((data_size, BUF_LENGTH, 1))\n",
        "y = np.empty((data_size, 2, 1))\n",
        "\n",
        "load_files(X, y, audio_paths, N_FILES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sPAgeAMOOZD"
      },
      "source": [
        "We then save the data to a npy file to speed up this process. The bottleneck is in the decoding of .mp3 files. We suggest to create this .npy file, upload them on your drive and load them directly from here on Colab (in place of the .mp3 files)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cawx8kGnOLeX"
      },
      "source": [
        "np.save('numpy_data/X', X)\n",
        "np.save('numpy_data/y', y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpsvbLjhOHqL"
      },
      "source": [
        "## Load directly data from .npy files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5z_R0IF4OFnl"
      },
      "source": [
        "X = np.load('numpy_data/X.npy')\n",
        "y = np.load('numpy_data/y.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FMgh24mcgw1"
      },
      "source": [
        "## Data augmentation (not applied but implemented)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pajhwtbQcfpL"
      },
      "source": [
        "def noiser(sample):\n",
        "  noise = np.random.normal(0,1, len(sample)) * 0.075\n",
        "  sample = sample + noise\n",
        "  return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8frTqC5UsTn"
      },
      "source": [
        "def pitch_shifter(sample, semitones):\n",
        "  return librosa.effects.pitch_shift(sample,44100,semitones)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQMFQPukoKSu"
      },
      "source": [
        "def data_augmenter(songs):\n",
        "  for s in songs:\n",
        "    audio, _ = librosa.load(path, sr=44100)\n",
        "    \n",
        "    noise = noiser(audio)\n",
        "    shift = pitch_shifter(audio, np.random.random_integers(-2,2))\n",
        "    id = Path(s).stem.split('.')[0]\n",
        "    sf.write(id+'-n'+'.wav', noise, 44100)\n",
        "    sf.write(id+'-ps'+'.wav', shift, 44100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynTqQfAbCCgO"
      },
      "source": [
        "# Create model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deVGgsQf5XEv"
      },
      "source": [
        "def get_model(batch_size):\n",
        "  input_shape = (22050,1)\n",
        "  input1 = Input(shape=input_shape, batch_size=batch_size, name='Input')\n",
        "  conv1 = Conv1D(8, 32, 8, activation='relu', padding='causal', kernel_regularizer=tf.keras.regularizers.l2(0.01)) (input1)\n",
        "  batch1 = BatchNormalization(name='BatchNorm1')(conv1)\n",
        "  mp1 = MaxPooling1D(8, padding='same')(batch1)\n",
        "\n",
        "  conv2 = Conv1D(8, 128, 32, activation='relu', padding='causal', kernel_regularizer=tf.keras.regularizers.l2(0.01))(input1)\n",
        "  batch2 = BatchNormalization(name='BatchNorm2')(conv2)\n",
        "  mp2 = MaxPooling1D(2, padding='same')(batch2)\n",
        "\n",
        "  add = Add(name='Add')([mp1, mp2])\n",
        "  drop1 = Dropout(0.2, name='Dropout1')(add)\n",
        "  lstm1 = Bidirectional(LSTM(32, return_sequences=True), name='BiLSTM1')(drop1)\n",
        "\n",
        "  lstm2 = Bidirectional(LSTM(32, return_sequences=False), name='BiLSTM2')(lstm1)\n",
        "  drop2 = Dropout(0.2)(lstm2)\n",
        "\n",
        "  output = Dense(2)(drop2)\n",
        "  model = tf.keras.Model(inputs=input1, outputs=output)\n",
        "\n",
        "  return model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vs1bjEuNCwI0"
      },
      "source": [
        "model = get_model(None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5eu7wm3PI7D"
      },
      "source": [
        "We created a model with a None batch size, now we compile it with an Adam optimizer. we can also plot it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NScZV1UY2yg1"
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=opt, loss=\"mse\", metrics=[ tf.keras.metrics.RootMeanSquaredError() ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15fNep7gmRcF"
      },
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rot7awsWYnFS"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxGRXlckPVAu"
      },
      "source": [
        "Before, we create a visualization plot function to see the loss behavior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d0qxWb6hlA2"
      },
      "source": [
        "def plot_history(history):\n",
        "  plt.figure(figsize=(15,5))\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(history.history[tf.keras.metrics.RootMeanSquaredError().name], c='r')\n",
        "  plt.title('Model RMSE')\n",
        "  plt.ylabel('RMSE')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend(('Training'))\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(history.history['loss'], c='r')\n",
        "  plt.title('Model loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend(('Training'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gol_VpZwPmro"
      },
      "source": [
        "The scheduler will be called after each epoch, and sometimes it will decrease the learning rate to help convergence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp2-PVvcPlhD"
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "  if epoch == 10 or epoch == 30 :\n",
        "    return lr * 0.1\n",
        "  return lr  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtc3XtpYPs0k"
      },
      "source": [
        "Let's also create a function to save in a .txt file the results of the training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjwaccvfPxOz"
      },
      "source": [
        "def print_results(r2, rmse):\n",
        "  output = '------------------------------------------------------------------------\\nScore Per fold:\\n'\n",
        "  for i in range(0, len(r2)):\n",
        "    output += '------------------------------------------------------------------------\\n'\n",
        "    output += f'> Fold {i+1} - R2: {r2[i]} - Rmse: {rmse[i]}%\\n'\n",
        "  output += '------------------------------------------------------------------------\\nAverage scores for all folds:\\n'\n",
        "  output += f'> R2: {np.mean(r2)}\\n'\n",
        "  output += f'> Rmse: {np.mean(rmse)} (+- {np.std(rmse)})\\n'\n",
        "  output += '------------------------------------------------------------------------'\n",
        "  print(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-y7doJzP0Dz"
      },
      "source": [
        "EPOCHS = 60\n",
        "N_SONGS_PER_BATCH = 8\n",
        "BATCH_SIZE = N_SONGS_PER_BATCH * 60\n",
        "PATIENCE = 20\n",
        "LEARNING_RATE = 0.0001\n",
        "N_FOLDS = 10\n",
        "\n",
        "output_folder = 'results'\n",
        "run_idx = 1\n",
        "\n",
        "scheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "early_stop = EarlyStopping('loss', patience=PATIENCE, restore_best_weights=True)\n",
        "\n",
        "kfold = KFold(n_splits=N_FOLDS, shuffle=False) \n",
        "r2=[]\n",
        "rmse=[]\n",
        "\n",
        "for idx_train, idx_test in kfold.split(X, y):\n",
        "    STEPS = int(( np.floor(len(idx_train))) // (BATCH_SIZE) )\n",
        "    print('Run index : ' + str(run_idx))\n",
        "\n",
        "    #Setup of csv logger\n",
        "    log_file_name = 'Run_' + str(run_idx)\n",
        "    run_idx+=1\n",
        "    csv_logger = CSVLogger(os.path.join(output_folder, log_file_name), append=False)\n",
        "\n",
        "    #create callbacklist\n",
        "    callbacks = [csv_logger, early_stop, scheduler_callback]\n",
        "    #init model\n",
        "    model = get_model2(None)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "    model.compile(optimizer=opt, loss=\"mse\", metrics=[ tf.keras.metrics.RootMeanSquaredError() ])\n",
        "\n",
        "    #Train and plot results\n",
        "    history = model.fit(X[idx_train], y[idx_train], \n",
        "    epochs=EPOCHS, \n",
        "    steps_per_epoch=STEPS, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    callbacks=callbacks, \n",
        "    verbose=1)\n",
        "\n",
        "    #metrics creation\n",
        "    scores = model.evaluate(X[idx_test], y[idx_test], verbose=2)\n",
        "    plot_history(history)\n",
        "    rmse.append(scores[1])\n",
        "    metric = tfa.metrics.r_square.RSquare(y_shape=(2,))\n",
        "    metric.update_state(tf.squeeze(y[idx_test]), model.predict(X[idx_test]))\n",
        "    result = metric.result()\n",
        "    r2.append(result.numpy())\n",
        "\n",
        "    gc.collect()\n",
        "    k.clear_session();\n",
        "\n",
        "\n",
        "print_results(r2, rmse)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvC9dJnZYqnG"
      },
      "source": [
        "# Model save"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv09FvIvi_qY"
      },
      "source": [
        "After having tuned the hyperparameters of the network using also validation data, we trained the model on the whole dataset and then saved it using the ModelCheckpoint callback from Keras API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRem5QKAxbM5"
      },
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='./checkpoint',\n",
        "    save_weights_only=True,\n",
        "    monitor='mse',\n",
        "    mode='max',\n",
        "    save_best_only=True)]\n",
        "  \n",
        "  history = model.fit(X, y, \n",
        "  epochs=EPOCHS, \n",
        "  steps_per_epoch=STEPS, \n",
        "  batch_size=BATCH_SIZE, \n",
        "  callbacks=[checkpoint], \n",
        "  verbose=1)\n",
        "\n",
        "\n",
        "model.save('best_model.hdf5', include_optimizer=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}